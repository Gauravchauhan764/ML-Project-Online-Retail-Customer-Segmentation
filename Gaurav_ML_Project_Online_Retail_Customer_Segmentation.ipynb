{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "-Kee-DAl2viO",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gauravchauhan764/ML-Project-Online-Retail-Customer-Segmentation/blob/main/Gaurav_ML_Project_Online_Retail_Customer_Segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - **Online Retail Customer Segmentation**\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Unsupervised\n",
        "##### **Contribution**    - Individual\n",
        "##### **Name**   - Gaurav Kumar"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project aims to extract meaningful insights from a UK-based non-store online retail dataset covering transactions from 01/12/2010 to 09/12/2011. The dataset encompasses various attributes such as Invoice Number, Product Code, Item Description, Quantity, Invoice Date, Unit Price, Customer ID, and Country. The primary goal is to identify customer segments and patterns to aid in understanding customer behavior and product sales trends for a company specializing in unique all-occasion gifts.\n",
        "\n",
        "**Data Exploration and Cleaning:**\n",
        "\n",
        "The initial phase involves thorough data exploration to understand the dataset's structure, data types, missing values, duplicates, and outliers. It is imperative to ensure data cleanliness for accurate analysis.\n",
        "\n",
        "**Exploratory Data Analysis (EDA):**\n",
        "\n",
        "The EDA phase comprises Univariate Analysis to visualize individual attribute distributions, Bivariate Analysis to explore relationships between variables, and Multivariate Analysis, potentially using clustering techniques, to uncover patterns among multiple variables. These analyses will unveil insights into customer buying patterns, popular products, and sales trends across different regions and customer segments.\n",
        "\n",
        "**Feature Engineering:**\n",
        "\n",
        "Feature engineering will be conducted to create additional features that could enhance model performance and provide deeper insights into customer behavior or product preferences. This step involves transforming categorical variables and creating new ones based on existing attributes.\n",
        "\n",
        "**Visualization:**\n",
        "\n",
        "The project requires the creation of at least 15 logical and meaningful charts following the \"UBM\" rule: Univariate, Bivariate, and Multivariate analyses. These visualizations will include histograms, boxplots, scatter plots, and other relevant charts to depict trends, correlations, and patterns within the data. Insights gained from these visualizations will be crucial in understanding customer preferences and potential impacts on business decisions.\n",
        "\n",
        "**Machine Learning Models:**\n",
        "\n",
        "Various machine learning algorithms will be implemented, such as clustering or classification models, to segment customers or predict sales trends. Evaluation metrics like accuracy, F1-score, or RMSE will be used to measure model performance. Cross-validation and hyperparameter tuning will optimize the models for better accuracy and predictive power.\n",
        "\n",
        "**Business Impact Analysis:**\n",
        "\n",
        "The project's insights, both from visualization and machine learning models, will be translated into potential business impacts. This analysis will demonstrate how the derived insights can influence business decisions, customer targeting strategies, inventory management, and overall operational efficiency.\n",
        "\n",
        "The project emphasizes well-structured, commented code, exception handling, and deployment-ready code as additional credit points. A meticulous documentation process will be undertaken, explaining the rationale behind each step, visual, model, and business impact analysis, ensuring clarity and ease of understanding for stakeholders and future use."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Provide your GitHub Link here."
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The project involves analyzing a big set of data that records all the sales made by a UK online store selling special gifts between December 1, 2010, and December 9, 2011. The goal is to figure out different groups of customers who buy these gifts. This store has a lot of customers who buy things in bulk, and the aim is to understand what these different types of customers prefer and how they shop."
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**InvoiceNo:**  A unique 6-digit number for each transaction. If it starts with 'c', it means the transaction was canceled.\n",
        "\n",
        "**StockCode:** A unique 5-digit number for each product sold.\n",
        "\n",
        "**Description:** The name of the product/item sold.\n",
        "\n",
        "**Quantity:** The number of items bought per transaction.\n",
        "\n",
        "**InvoiceDate:** The date and time of each transaction.\n",
        "\n",
        "**UnitPrice:** The price per unit of the product in sterling.\n",
        "\n",
        "**CustomerID:** A unique 5-digit number assigned to each customer.\n",
        "\n",
        "**Country:** The country where the customer resides."
      ],
      "metadata": {
        "id": "fTZ9ddFqm3SD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing common libraries\n",
        "import numpy as np  # Handling arrays\n",
        "import pandas as pd  # Data manipulation, read_excel\n",
        "from numpy import math\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy import stats\n",
        "from datetime import datetime\n",
        "from pylab import rcParams\n",
        "import warnings\n",
        "\n",
        "# Set seaborn settings\n",
        "sns.set()\n",
        "warnings.filterwarnings('ignore')\n"
      ],
      "metadata": {
        "id": "JYAYwgLa7_wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the CSV file into a DataFrame\n",
        "df = pd.read_csv(\"/Online Retail.xlsx - Online Retail.csv\")"
      ],
      "metadata": {
        "id": "spGIU_599M46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "03DaRrbj9otb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "RQJCTw3G-L43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(10)  # This will display the first ten rows of the DataFrame \"df\""
      ],
      "metadata": {
        "id": "YFBW5XSH-7wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail()"
      ],
      "metadata": {
        "id": "M05HM-zv_pv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail(10)  # This will display the last ten rows of the DataFrame \"df\""
      ],
      "metadata": {
        "id": "c-sicyvc_1E-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your DataFrame\n",
        "shape_of_df = df.shape\n",
        "print(\"Number of rows:\", shape_of_df[0])\n",
        "print(\"Number of columns:\", shape_of_df[1])"
      ],
      "metadata": {
        "id": "lko5KwMF_AKd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying information about the DataFrame\n",
        "df.info()"
      ],
      "metadata": {
        "id": "ErhCd4Nk_hiT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_rows = df.duplicated().sum()\n",
        "print(\"Number of duplicate rows:\", duplicate_rows)"
      ],
      "metadata": {
        "id": "RK19pTKxAwkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count missing values in each column\n",
        "missing_values = df.isnull().sum()\n",
        "\n",
        "# Display the count of missing values for each column\n",
        "print(\"Missing Values/Null Values Count:\")\n",
        "print(missing_values)"
      ],
      "metadata": {
        "id": "EJ-WB_sfA41g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a heatmap to visualize Missing Values/Null Values in the DataFrame\n",
        "plt.figure(figsize=(8, 6))  # Set the figure size\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')  # Plot the heatmap with 'viridis' color map\n",
        "plt.title('Null Values Heatmap')  # Set the title for the plot\n",
        "plt.show()  # Display the plot"
      ],
      "metadata": {
        "id": "J8lkU89WBL8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Attributes Information:**\n",
        "\n",
        "InvoiceNo: An identifier for each transaction. If the code starts with 'c', it indicates a cancellation.\n",
        "\n",
        "StockCode: Product code, uniquely assigned to each distinct product.\n",
        "Description: Name of the product.\n",
        "\n",
        "Quantity: The quantity of each product per transaction.\n",
        "\n",
        "InvoiceDate: Date and time when each transaction was generated.\n",
        "\n",
        "UnitPrice: Price per unit of the product in sterling.\n",
        "\n",
        "CustomerID: Unique identifier for each customer.\n",
        "\n",
        "Country: Name of the country where each customer resides.\n",
        "\n",
        "**Nature of Data:**\n",
        "\n",
        "Nominal Attributes: InvoiceNo, StockCode, Description, CustomerID, and Country are categorical attributes.\n",
        "\n",
        "Numeric Attributes: Quantity and UnitPrice are numerical attributes.\n",
        "Temporal Data: InvoiceDate provides temporal information about transactions.\n",
        "\n",
        "**Dataset Size and Characteristics:**\n",
        "\n",
        "Size: The dataset contains records of transactions occurring between 01/12/2010 and 09/12/2011.\n",
        "\n",
        "Type of Transactions: It captures details of purchases and potential cancellations.\n",
        "\n",
        "Geographical Scope: It covers customers residing in various countries, although it primarily focuses on the UK-based retail company.\n",
        "\n"
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "column_names = df.columns\n",
        "print(column_names)"
      ],
      "metadata": {
        "id": "OaPCFO6UDOip"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display columns and data types\n",
        "columns_data_types = pd.DataFrame(df.dtypes).rename(columns={'dtype': 'Data Type'})\n",
        "print(columns_data_types)"
      ],
      "metadata": {
        "id": "jnH6v-uEDBwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "id": "XH6bqdAGELgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**InvoiceNo:** An invoice number, a 6-digit integral number uniquely assigned to each transaction. If it begins with the letter 'c', it indicates a cancellation.\n",
        "\n",
        "**StockCode:** Product (item) code, a 5-digit integral number uniquely assigned to each distinct product.\n",
        "\n",
        "**Description:** The name of the product (item).\n",
        "\n",
        "**Quantity:** The quantity of each product (item) per transaction, a numeric value.\n",
        "\n",
        "**InvoiceDate:** Date and time of the invoice, in numeric format, representing the day and time when each transaction was generated.\n",
        "\n",
        "**UnitPrice:** Unit price of the product, a numeric value representing the price per unit in sterling.\n",
        "\n",
        "**CustomerID:** Customer number, a 5-digit integral number uniquely assigned to each customer.\n",
        "\n",
        "**Country:** The name of the country where each customer resides, nominal categorical data."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display unique values for each column\n",
        "for column in df.columns:\n",
        "    unique_values = df[column].unique()\n",
        "    print(f\"Unique values for '{column}': {len(unique_values)}\\n{unique_values}\\n\")"
      ],
      "metadata": {
        "id": "iW0JlDNQE6Kv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready."
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying column names\n",
        "print(df.columns.tolist())"
      ],
      "metadata": {
        "id": "iLmtQP8aF4VB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identifying numerical columns\n",
        "numerical_columns = df.select_dtypes(['int64', 'float64']).columns.tolist()\n",
        "numerical_features = pd.Index(numerical_columns)\n",
        "numerical_features"
      ],
      "metadata": {
        "id": "WRuSTyZMGOL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify categorical columns in the DataFrame\n",
        "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
        "categorical_features = pd.Index(categorical_columns)\n",
        "categorical_features\n"
      ],
      "metadata": {
        "id": "RnmJNOTIGaW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def unique_values_and_count(column):\n",
        "    unique_vals = df[column].unique()\n",
        "    num_unique_vals = df[column].nunique()\n",
        "    print(f\"Unique values in {column}: {unique_vals}\")\n",
        "    print(f\"Number of unique values in {column}: {num_unique_vals}\")\n",
        "\n",
        "# Assuming 'categorical_columns' contains the names of categorical columns\n",
        "for col in categorical_columns:\n",
        "    print(col.upper())\n",
        "    unique_values_and_count(col)"
      ],
      "metadata": {
        "id": "tzs4hY5JGaEk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_rows = df[df.duplicated()]\n",
        "num_duplicate_rows = len(duplicate_rows)\n",
        "num_duplicate_rows"
      ],
      "metadata": {
        "id": "oBlnGPU1HJpf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_records = df[df.duplicated()]\n",
        "duplicate_records"
      ],
      "metadata": {
        "id": "VeOlwIeuHJXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove duplicate rows from the DataFrame\n",
        "df.drop_duplicates(inplace=True)\n",
        "\n",
        "# Check for null values in the DataFrame\n",
        "null_values = df.isnull().sum()\n",
        "print(null_values)"
      ],
      "metadata": {
        "id": "2xDA4O-uHjlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Missing value imputation**"
      ],
      "metadata": {
        "id": "b8GEHbshH_sB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dropping missing values from the DataFrame 'df'\n",
        "df.dropna(inplace=True)"
      ],
      "metadata": {
        "id": "gL4pBeN4H_Hi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for null values in the DataFrame\n",
        "null_counts = df.isnull().sum()\n",
        "\n",
        "# Display the count of null values for each column\n",
        "print(null_counts)"
      ],
      "metadata": {
        "id": "aRRU5uClIdWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'df' is the DataFrame containing your data\n",
        "# Removing rows with null values\n",
        "df_clean = df.dropna()\n",
        "\n",
        "# Finding the number of records remaining after removing null values\n",
        "remaining_records = df_clean.shape[0]\n",
        "print(\"Number of records after removing null values:\", remaining_records)\n"
      ],
      "metadata": {
        "id": "1RYM1hgzIrUz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a heatmap to visualize Missing Values/Null Values in the DataFrame\n",
        "plt.figure(figsize=(8, 6))  # Set the figure size\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='viridis')  # Plot the heatmap with 'viridis' color map\n",
        "plt.title('Null Values Heatmap')  # Set the title for the plot\n",
        "plt.show()  # Display the plot"
      ],
      "metadata": {
        "id": "9mStAyFkIqp8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking for null values\n",
        "df.info()"
      ],
      "metadata": {
        "id": "N4WpQCiEJk0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We need to remove certain 'InvoiceNo' entries that start with 'C' because this letter indicates a canceled transaction."
      ],
      "metadata": {
        "id": "0CaffpE2KU9z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert 'InvoiceNo' column to string data type\n",
        "df['InvoiceNo'] = df['InvoiceNo'].astype(str)\n",
        "df['InvoiceNo']\n"
      ],
      "metadata": {
        "id": "-QeM8IE3Jycz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter out rows where InvoiceNo does not contain 'C'\n",
        "df = df[~df['InvoiceNo'].str.startswith('C')]\n",
        "df\n",
        "\n"
      ],
      "metadata": {
        "id": "98Lsvm-WJ52p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming df is your DataFrame\n",
        "shape_of_df = df.shape\n",
        "print(\"Number of rows:\", shape_of_df[0])\n",
        "print(\"Number of columns:\", shape_of_df[1])"
      ],
      "metadata": {
        "id": "aiAwTOCLKhPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Displaying summary statistics for numerical columns in the DataFrame\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "m8-n3NXdKmHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Column Identification:** Identified numerical columns (Quantity, UnitPrice, CustomerID) and categorical columns (InvoiceNo, StockCode, Description, InvoiceDate, Country).\n",
        "\n",
        "**Handling Duplicate Rows:**\n",
        "\n",
        "Detected and removed duplicate rows in the dataset, resulting in 1709 duplicate records being removed.\n",
        "\n",
        "**Handling Missing Values:**\n",
        "\n",
        "Identified missing values in columns (Description, UnitPrice, CustomerID, Country).\n",
        "\n",
        "Removed rows with any missing values, resulting in a dataset of 144518 records.\n",
        "\n",
        "**Null Value Visualization:** Created a heatmap to visualize the distribution of missing values across the dataset.\n",
        "\n",
        "**Data Type Transformation:** Converted InvoiceNo to string data type.\n",
        "\n",
        "**Filtering Data:** Filtered out rows with InvoiceNo starting with 'C', assuming these represent canceled transactions.\n",
        "\n",
        "**Summary Statistics:** Displayed summary statistics (count, mean, min, max, etc.) for numerical columns using df.describe()."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1.  Visualize the count of unique values for different features."
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting unique values of features\n",
        "unique_df = pd.DataFrame()\n",
        "unique_df['Features'] = df.columns\n",
        "unique = [df[i].nunique() for i in df.columns]\n",
        "unique_df['Uniques'] = unique"
      ],
      "metadata": {
        "id": "JKV1XGtsVqYl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#visualization code\n",
        "plt.figure(figsize=(10, 5))\n",
        "splot = sns.barplot(x=unique_df['Features'], y=unique_df['Uniques'], alpha=0.8)\n",
        "for p in splot.patches:\n",
        "    splot.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()), ha='center',\n",
        "                   va='center', xytext=(0, 9), textcoords='offset points')\n",
        "plt.title('Bar plot for number of unique values in each column', weight='bold', size=15)\n",
        "plt.ylabel('Unique values', size=12, weight='bold')\n",
        "plt.xlabel('Features', size=12, weight='bold')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rDGI5hlDVF1N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The bar chart was chosen to visualize the count of unique values in each column because it provides a clear comparison of the diversity within different features, aiding in understanding the variability and distribution across the dataset at a glance."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The chart displays the count of unique values for each feature in the dataset. It helps identify the variability and distinctiveness within each attribute. The insights reveal the diversity in the dataset, showcasing the range of different values present in each column, aiding in understanding the data's richness and potential complexity."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The visualization displaying unique values per feature helps in understanding data granularity, potentially aiding in targeted marketing strategies. However, if certain features show limited diversity (few unique values), it might limit customer segmentation accuracy, potentially impacting marketing personalization negatively."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2. Visualizing the count of description names."
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting unique values of Description names with higher counts at the top\n",
        "Description_df=df['Description'].value_counts().reset_index()\n",
        "Description_df.rename(columns={'index': 'Description_Name'}, inplace=True)\n",
        "Description_df.rename(columns={'Description': 'Count'}, inplace=True)\n",
        "Description_df.head()\n"
      ],
      "metadata": {
        "id": "LV8NiKTLXKIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization: Count of Product Names\n",
        "plt.figure(figsize=(18, 6))\n",
        "plt.title('Top 5 Product Names and Their Counts')\n",
        "top_products = Description_df[:5]  # Assuming Description_df contains product names and their counts\n",
        "sns.barplot(x='Description_Name', y='Count', data=top_products)\n",
        "plt.xlabel('Product Name')\n",
        "plt.ylabel('Count')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "3oVi2ZGEXJyf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I selected the bar chart to display the top 5 product names and their respective counts because it offers a clear comparison of the most popular products based on their frequency in the dataset. The visualization helps identify the best-selling items efficiently."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The chart displays the top 5 most frequently sold product names, showing their respective counts. It provides insight into the most popular products in terms of sales volume within the dataset."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The top 5 products by count indicate high demand, potentially allowing targeted marketing strategies to boost sales and customer engagement, positively impacting the business. However, if certain products exhibit declining counts over time, it might signify reduced popularity, requiring inventory or marketing adjustments to prevent negative impacts on sales and revenue"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3. Visualizing the tail of description names (bottom 5 product names)."
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#description name\n",
        "Description_df.tail()"
      ],
      "metadata": {
        "id": "BWNRp-UjarkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing the tail of description names (bottom 5 product names)\n",
        "bottom_5_description = df['Description'].value_counts().tail(5)\n",
        "plt.figure(figsize=(8, 6))\n",
        "bottom_5_description.plot(kind='barh')\n",
        "plt.title('Bottom 5 Product Names')\n",
        "plt.xlabel('Frequency')\n",
        "plt.ylabel('Product Name')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nI7jGpRjaU94"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The specific chart chosen was a horizontal bar chart (barh) to display the bottom 5 product names based on their frequencies. This choice allows for a clear comparison of less frequent product names, aiding in easy identification of the least common items in the dataset."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The visualization shows the least frequent product names in the dataset, indicating items with the lowest occurrences in transactions. These bottom 5 products have notably lower frequency compared to the rest, possibly representing niche or less popular items in the retail inventory."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The visualization displays the least frequent product names. Understanding these less common items can aid in identifying slow-moving or niche products. This insight could guide inventory management, potentially reducing overstocking of less popular items and reallocating resources to higher-selling products, leading to a positive business impact by optimizing inventory. However, if these products represent discontinued or seasonal items, their low frequency might not necessarily indicate negative growth but rather strategic sales patterns.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4. Visualize the count of stock names."
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of each stock code, with higher counts displayed first\n",
        "StockCode_df=df['StockCode'].value_counts().reset_index()\n",
        "StockCode_df.rename(columns={'index': 'StockCode_Name'}, inplace=True)\n",
        "StockCode_df.rename(columns={'StockCode': 'Count'}, inplace=True)\n",
        "StockCode_df.head()\n"
      ],
      "metadata": {
        "id": "p24dzhdWbrQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualize the count of stock names.\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.title('Top 5 Stock Names and Counts')\n",
        "top_stock_names = StockCode_df[:5]  # Assuming StockCode_df contains the necessary data\n",
        "sns.barplot(x='StockCode_Name', y='Count', data=top_stock_names)\n",
        "plt.xlabel('Stock Names')\n",
        "plt.ylabel('Count')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wyGt3IFCbq9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The bar plot was chosen to display the top 5 stock names and their respective counts as it offers a clear visual representation of the frequency of each stock name. This visualization helps quickly identify the most common stock names in the dataset, aiding in understanding the distribution of products sold."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The chart displays the top 5 stock names based on their counts in the dataset. It offers insight into the most frequently occurring stock names, highlighting the distribution of stock items by their occurrence frequency."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The visualization showcases the top 5 stock names based on their counts. Understanding popular stock items can guide inventory management, potentially improving stock availability for high-demand items and enhancing customer satisfaction. However, if certain popular items face supply chain issues or stock shortages, it could lead to negative growth due to unmet customer demand, impacting sales and customer retention."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5. Visualize the count of stock names for the bottom 5 items"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stock code name\n",
        "StockCode_df.tail()"
      ],
      "metadata": {
        "id": "NBAOduxoeR0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization for bottom 5 stock names\n",
        "plt.figure(figsize=(13, 5))\n",
        "plt.title('Bottom 5 Stock Names')\n",
        "sns.barplot(x='StockCode_Name', y='Count', data=StockCode_df[-5:])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "bDcVePYteCqn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart, a bar plot of the bottom 5 stock names and their counts, was chosen to highlight the least frequently sold items among the stock. This visualization helps identify the less popular products in the dataset, aiding in understanding inventory turnover and potentially informing restocking or marketing strategies."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The chart displays the least occurring stock names in the dataset. It highlights the infrequently purchased items among the products sold, potentially indicating less popular or niche items within the inventory."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The visualization of the bottom 5 stock names suggests their relatively lower transaction counts compared to others. While understanding less popular stock items may aid inventory management, focusing solely on these might lead to missed opportunities in promoting potentially profitable products, potentially impacting business positively or negatively based on how this information is utilized."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6. Visualization of 'Top 5 Country based Most Numbers of  Customers'"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Counting unique value of country_name as higher count comes first\n",
        "country_df=df['Country'].value_counts().reset_index()\n",
        "country_df.rename(columns={'index': 'Country_Name'}, inplace=True)\n",
        "country_df.rename(columns={'Country': 'Count'}, inplace=True)\n",
        "country_df.head()"
      ],
      "metadata": {
        "id": "Wq7ekrdDgAE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of 'Top 5 Countries with the Most Number of Customers'\n",
        "plt.figure(figsize=(13,6))\n",
        "plt.title('Top 5 Country based on the Most Numbers Customers')\n",
        "sns.barplot(x='Country_Name',y='Count',data=country_df[:5])"
      ],
      "metadata": {
        "id": "ocTutLNCgoFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I chose a bar chart to illustrate the top 5 countries with the most customers because it offers a clear comparison among countries based on customer count. This visualization helps identify the countries contributing the most to the customer base, aiding in understanding the distribution of customers across different regions."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The chart illustrates that the top 5 countries with the highest number of customers in the dataset are displayed, with Country_Name on the x-axis and the corresponding Count of customers on the y-axis. This insight helps identify the primary countries contributing to the customer base of the online retail company.\n",
        "\n"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The insights from the visualization of the top 5 countries by customer count can aid in targeting marketing strategies towards regions with higher customer engagement, potentially leading to increased sales and business growth. However, if certain countries exhibit a significantly lower customer count, it might indicate untapped markets or issues with customer retention strategies, possibly impacting expansion efforts negatively and requiring further investigation for better market penetration."
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7. Visualizing the top 5 countries based on the least number of customers"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the last few rows of the DataFrame\n",
        "country_df.tail()"
      ],
      "metadata": {
        "id": "RrfVFTJ6iXhi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Visualizing the top 5 countries based on the least number of customers\n",
        "plt.figure(figsize=(13,5))\n",
        "plt.title('Top 5 Country based least Numbers of  Customers')\n",
        "sns.barplot(x='Country_Name',y='Count',data=country_df[-5:])"
      ],
      "metadata": {
        "id": "LGUwvA4kiN19"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The specific chart, a bar plot showcasing the top 5 countries with the least customer count, was chosen to highlight the distribution of customer numbers across different countries in the dataset."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The visualization reveals the countries with the least customer counts. It highlights the nations with the fewest customer engagements in the dataset, offering a clear ranking of the five countries at the bottom in terms of customer participation."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The visualization of the top 5 countries with the fewest customers indicates that these regions have considerably lower customer counts. Understanding this distribution can aid in targeted marketing strategies to boost customer engagement in these countries, potentially expanding the business reach.\n",
        "\n",
        "However, a notable observation from this insight is the comparatively low customer base in these specific countries. Focusing solely on these markets might not yield immediate substantial growth without considering additional factors like market demand or cultural preferences. Therefore, while it offers an opportunity for expansion, the concentration solely on these regions might not guarantee rapid business growth."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis (H0):** The average quantity of items sold in the UK is the same as in France.\n",
        "\n",
        "**Alternate Hypothesis (H1):** The average quantity of items sold in the UK is different from that in France."
      ],
      "metadata": {
        "id": "wKv5E2-vn7es"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import ttest_ind\n",
        "\n",
        "# Extracting data for the UK and France\n",
        "data_uk = df[df['Country'] == 'United Kingdom']['Quantity']\n",
        "data_fr = df[df['Country'] == 'France']['Quantity']\n",
        "\n",
        "# Performing t-test\n",
        "t_stat, p_value = ttest_ind(data_uk, data_fr, equal_var=False)\n",
        "t_stat, p_value\n"
      ],
      "metadata": {
        "id": "JERKu7hiozNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conduct a two-sample t-test to compare the means of the quantity of items sold between the UK and France.\n",
        "\n",
        "The p-value of approximately 0.154 suggests weak evidence against the null hypothesis. We fail to reject the null hypothesis, indicating no significant difference in the average quantity of items sold between the UK and France."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The two-sample t-test is chosen because we're comparing the means of two independent groups (UK and France) to determine if they're significantly different."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis (H0):** There is no significant difference in the unit prices of top 5 products compared to the bottom 5 products.\n",
        "\n",
        "**Alternate Hypothesis (H1):** There is a significant difference in the unit prices of top 5 products compared to the bottom 5 products."
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Subset data for top 5 and bottom 5 products\n",
        "top_5_unit_price = df[df['Description'].isin(top_products['Description_Name'])]['UnitPrice']\n",
        "bottom_5_unit_price = df[df['Description'].isin(bottom_5_description.index)]['UnitPrice']\n",
        "\n",
        "# Perform t-test\n",
        "t_stat, p_value = ttest_ind(top_5_unit_price, bottom_5_unit_price, equal_var=False)\n",
        "t_stat, p_value"
      ],
      "metadata": {
        "id": "qzdrgFbUqT6J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Independent samples t-test to compare the means of unit prices between top 5 and bottom 5 products.\n",
        "\n",
        " The p-value of approximately 0.313 suggests weak evidence against the null hypothesis. We fail to reject the null hypothesis, indicating no significant difference in the unit prices between the top 5 and bottom 5 products."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "An independent samples t-test is suitable here as we're comparing the means of two independent groups (top 5 and bottom 5 products) to determine if they significantly differ."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Null Hypothesis (H0):** There is no correlation between quantity and unit price of items sold.\n",
        "\n",
        "**Alternate Hypothesis (H1):** There is a correlation between quantity and unit price of items sold."
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Calculate Pearson correlation\n",
        "correlation_coefficient, p_value = pearsonr(df['Quantity'], df['UnitPrice'])\n",
        "correlation_coefficient, p_value"
      ],
      "metadata": {
        "id": "_RKFjN3pq4cL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pearson correlation coefficient to measure the linear correlation between quantity and unit price.\n",
        "\n",
        "The correlation coefficient of approximately -0.004 suggests a very weak negative linear relationship between quantity and unit price. The p-value of approximately 0.128 indicates weak evidence against the null hypothesis. Therefore, we fail to reject the null hypothesis, suggesting no significant correlation between quantity and unit price of items sold."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Pearson correlation test is used to measure the strength and direction of the linear relationship between two continuous variables (quantity and unit price)."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling missing values\n",
        "# For 'Description' column, replace missing values with 'Unknown'\n",
        "df['Description'].fillna('Unknown', inplace=True)\n",
        "\n",
        "# For 'UnitPrice' column, fill missing values with the median\n",
        "median_unit_price = df['UnitPrice'].median()\n",
        "df['UnitPrice'].fillna(median_unit_price, inplace=True)\n",
        "\n",
        "# For 'CustomerID' column, drop rows with missing values as CustomerID is crucial\n",
        "df.dropna(subset=['CustomerID'], inplace=True)\n"
      ],
      "metadata": {
        "id": "49pQ4YvtwXQR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replacing missing 'Description' values with 'Unknown' to avoid losing information.\n",
        "\n",
        "Filling missing 'UnitPrice' values with the median to maintain the column's statistical properties.\n",
        "\n",
        "Dropping rows with missing 'CustomerID' as this is crucial for customer-related analysis."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling outliers\n",
        "from scipy.stats import zscore\n",
        "\n",
        "# Use z-score to identify and remove outliers in 'Quantity' and 'UnitPrice' columns\n",
        "z_scores = zscore(df[['Quantity', 'UnitPrice']])\n",
        "abs_z_scores = np.abs(z_scores)\n",
        "filtered_entries = (abs_z_scores < 3).all(axis=1)\n",
        "df = df[filtered_entries]\n"
      ],
      "metadata": {
        "id": "hfyUn8boxJId"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Employing z-score method to identify outliers and remove entries where 'Quantity' and 'UnitPrice' fall outside 3 standard deviations."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding categorical columns using Label Encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "df['Country_encoded'] = label_encoder.fit_transform(df['Country'])\n"
      ],
      "metadata": {
        "id": "Y2mk3sUyxP8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Utilizing Label Encoding for 'Country' column to convert country names into numerical labels."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Sample text\n",
        "text = \"Expand contraction, lower casing, removing punctuations... This is a sample text! Visit www.sampleurl.com.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "2nc8P3ml6bxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contractions_dict = {\n",
        "    \"ain't\": \"am not\",\n",
        "    \"aren't\": \"are not\",\n",
        "    \"can't\": \"cannot\",\n",
        "    \"can't've\": \"cannot have\",\n",
        "    \"could've\": \"could have\",\n",
        "    \"couldn't\": \"could not\",\n",
        "    \"didn't\": \"did not\",\n",
        "    \"doesn't\": \"does not\",\n",
        "    \"don't\": \"do not\",\n",
        "    \"hadn't\": \"had not\",\n",
        "    \"hasn't\": \"has not\",\n",
        "    \"haven't\": \"have not\",\n",
        "    \"he'd\": \"he would\",\n",
        "    \"he'll\": \"he will\",\n",
        "    \"he's\": \"he is\",\n",
        "    \"how'd\": \"how did\",\n",
        "    \"how'll\": \"how will\",\n",
        "    \"how's\": \"how is\",\n",
        "    \"I'd\": \"I would\",\n",
        "    \"I'll\": \"I will\",\n",
        "    \"I'm\": \"I am\",\n",
        "    \"I've\": \"I have\",\n",
        "    \"isn't\": \"is not\",\n",
        "    \"it'd\": \"it would\",\n",
        "    \"it'll\": \"it will\",\n",
        "    \"it's\": \"it is\",\n",
        "    \"let's\": \"let us\",\n",
        "    \"ma'am\": \"madam\",\n",
        "    \"mayn't\": \"may not\",\n",
        "    \"might've\": \"might have\",\n",
        "    \"mightn't\": \"might not\",\n",
        "    \"must've\": \"must have\",\n",
        "    \"mustn't\": \"must not\",\n",
        "    \"needn't\": \"need not\",\n",
        "    \"oughtn't\": \"ought not\",\n",
        "    \"shan't\": \"shall not\",\n",
        "    \"she'd\": \"she would\",\n",
        "    \"she'll\": \"she will\",\n",
        "    \"she's\": \"she is\",\n",
        "    \"should've\": \"should have\",\n",
        "    \"shouldn't\": \"should not\",\n",
        "    \"that's\": \"that is\",\n",
        "    \"there's\": \"there is\",\n",
        "    \"they'd\": \"they would\",\n",
        "    \"they'll\": \"they will\",\n",
        "    \"they're\": \"they are\",\n",
        "    \"they've\": \"they have\",\n",
        "    \"wasn't\": \"was not\",\n",
        "    \"we'd\": \"we would\",\n",
        "    \"we'll\": \"we will\",\n",
        "    \"we're\": \"we are\",\n",
        "    \"we've\": \"we have\",\n",
        "    \"weren't\": \"were not\",\n",
        "    \"what'll\": \"what will\",\n",
        "    \"what're\": \"what are\",\n",
        "    \"what's\": \"what is\",\n",
        "    \"what've\": \"what have\",\n",
        "    \"where's\": \"where is\",\n",
        "    \"who'd\": \"who would\",\n",
        "    \"who'll\": \"who will\",\n",
        "    \"who're\": \"who are\",\n",
        "    \"who's\": \"who is\",\n",
        "    \"who've\": \"who have\",\n",
        "    \"won't\": \"will not\",\n",
        "    \"wouldn't\": \"would not\",\n",
        "    \"you'd\": \"you would\",\n",
        "    \"you'll\": \"you will\",\n",
        "    \"you're\": \"you are\",\n",
        "    \"you've\": \"you have\"\n",
        "}\n",
        "\n",
        "def expand_contractions(text, contractions_dict):\n",
        "    words = text.split()\n",
        "    expanded_text = [contractions_dict[word] if word in contractions_dict else word for word in words]\n",
        "    return ' '.join(expanded_text)\n",
        "\n",
        "# Example usage:\n",
        "sample_text = \"I don't like it. You're amazing!\"\n",
        "expanded_text = expand_contractions(sample_text, contractions_dict)\n",
        "print(\"Original Text:\", sample_text)\n",
        "print(\"Expanded Text:\", expanded_text)\n"
      ],
      "metadata": {
        "id": "gnSID2XJpBJz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lowercasing\n",
        "text = text.lower()\n",
        "\n",
        "print(\"Lowercasing:\", text)"
      ],
      "metadata": {
        "id": "AwViJfL88Fd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing Punctuations\n",
        "text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "print(\"Removing Punctuations:\", text)"
      ],
      "metadata": {
        "id": "y-TDnrvm8RNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing URLs and words containing digits\n",
        "text = re.sub(r'http\\S+|www\\S+|\\S+\\d+\\S+', '', text)\n",
        "\n",
        "print(\"Removing URLs & Words with Digits:\", text)"
      ],
      "metadata": {
        "id": "Ncy-Ma3c8coH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Manipulation\n",
        "# Creating a new feature 'TotalPrice' by multiplying 'Quantity' and 'UnitPrice'\n",
        "df['TotalPrice'] = df['Quantity'] * df['UnitPrice']\n",
        "\n",
        "# Feature Selection\n",
        "# Considering 'Quantity', 'UnitPrice', 'Country_encoded', and 'TotalPrice' as important features\n",
        "selected_features = ['Quantity', 'UnitPrice', 'Country_encoded', 'TotalPrice']\n"
      ],
      "metadata": {
        "id": "9fI6_5Rdyuln"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "# Assuming the dataset is loaded into a DataFrame named 'df'\n",
        "\n",
        "# Displaying column names\n",
        "print(df.columns.tolist())\n",
        "\n",
        "# Identifying numerical columns\n",
        "numerical_columns = df.select_dtypes(['int64', 'float64']).columns.tolist()\n",
        "numerical_features = pd.Index(numerical_columns)\n",
        "print(\"Numerical Features:\", numerical_features)\n",
        "\n",
        "# Identify categorical columns in the DataFrame\n",
        "categorical_columns = df.select_dtypes(include=['object']).columns.tolist()\n",
        "categorical_features = pd.Index(categorical_columns)\n",
        "print(\"Categorical Features:\", categorical_features)\n",
        "\n",
        "# Dropping irrelevant columns (if any)\n",
        "# Assuming 'irrelevant_columns' contains names of columns deemed irrelevant\n",
        "#df = df.drop(columns=irrelevant_columns)\n",
        "\n",
        "# Handling missing values\n",
        "# Drop rows with any missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Convert 'InvoiceNo' column to string data type\n",
        "df['InvoiceNo'] = df['InvoiceNo'].astype(str)\n",
        "\n",
        "# Filter out rows where InvoiceNo does not contain 'C'\n",
        "df = df[~df['InvoiceNo'].str.startswith('C')]\n",
        "\n",
        "# Displaying summary statistics for numerical columns in the DataFrame\n",
        "print(df.describe())\n",
        "#"
      ],
      "metadata": {
        "id": "gs4zgU2r_Fn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecting relevant features\n",
        "selected_features = ['InvoiceNo', 'StockCode', 'Description', 'Quantity', 'InvoiceDate', 'UnitPrice', 'Country']\n",
        "selected_df = df[selected_features]\n",
        "\n",
        "# Checking the shape of the selected DataFrame\n",
        "print(\"Shape of selected DataFrame:\", selected_df.shape)\n"
      ],
      "metadata": {
        "id": "4Jt9A3f_AD-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Correlation Analysis: Identifying features highly correlated with the target or with each other. High correlation might indicate redundancy.\n",
        "\n",
        "Variance Thresholding: Removing features with low variance, assuming they hold little information for the model.\n",
        "\n",
        "Recursive Feature Elimination (RFE): Iteratively selecting features by training models and eliminating the least important ones.\n",
        "\n",
        "Feature Importance from Models: Utilizing algorithms like Random Forest, Gradient Boosting, or XGBoost to derive feature importance scores.\n",
        "\n",
        "SelectKBest: Using statistical tests (e.g., chi-square for classification, ANOVA for regression) to select the K best features based on their statistical significance.\n",
        "\n",
        "L1 Regularization (LASSO): Encouraging sparsity by penalizing the absolute size of coefficients, effectively setting some coefficients to zero."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upon analyzing the Online Retail Customer Segmentation data, the following important features were identified:\n",
        "\n",
        "Country: It might help understand regional preferences or trends in customer behavior.\n",
        "\n",
        "Quantity: Indicates the number of items purchased, potentially indicative of customer spending habits or order size.\n",
        "\n",
        "Unit Price: Reflects the price of individual items, crucial for understanding product pricing strategies and customer preferences.\n",
        "\n",
        "Description: Product descriptions can aid in understanding the type and category of items purchased, contributing to market basket analysis.\n",
        "\n",
        "Feature importance could vary based on the specific goals of analysis, model requirements, and the nature of the dataset. The mentioned features were considered important due to their potential impact on understanding customer behavior, market trends, and their direct relevance to the context of retail customer segmentation"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the data has been transformed for several reasons:\n",
        "\n",
        "Handling Missing Values:  Dropping rows with missing values ensures cleaner data for analysis without imputing or assuming values for missing entries.\n",
        "\n",
        "Filtering Cancelled Transactions: Removing transactions starting with 'C' in the InvoiceNo column helps exclude potentially problematic or reversed orders.\n",
        "\n",
        "Categorical to Numeric Conversion: Converting categorical columns like 'Country' and 'Description' to numeric formats using Label Encoding aids in numerical analysis and modeling by assigning unique identifiers to categorical values.\n",
        "\n",
        "Numerical Data Scaling: Scaling numerical columns ('Quantity', 'UnitPrice') using StandardScaler helps bring features to a similar scale, preventing one feature from dominating in models sensitive to feature magnitudes.\n",
        "\n",
        "These transformations address data quality, aid in analysis, and prepare the dataset for segmentation or modeling tasks by handling missing values, converting categorical data, and scaling numerical features."
      ],
      "metadata": {
        "id": "rxj8Y9SiBb2W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "# Assuming the dataset is loaded into a DataFrame named 'df'\n",
        "\n",
        "# Handling missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Filter out rows with InvoiceNo starting with 'C' (assuming 'InvoiceNo' is a string type)\n",
        "df = df[~df['InvoiceNo'].str.startswith('C')]\n",
        "\n",
        "# Convert 'InvoiceNo' column to string data type\n",
        "df['InvoiceNo'] = df['InvoiceNo'].astype(str)\n",
        "\n",
        "# Convert categorical columns to numeric using Label Encoding or One-Hot Encoding\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "categorical_columns = ['Country', 'Description']  # Assuming these are categorical columns\n",
        "\n",
        "for col in categorical_columns:\n",
        "    df[col] = label_encoder.fit_transform(df[col])\n",
        "\n",
        "# You can also use One-Hot Encoding for categorical columns\n",
        "# df = pd.get_dummies(df, columns=categorical_columns)\n",
        "\n",
        "# Optionally, you can perform feature scaling on numerical columns (Quantity, UnitPrice) if needed\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "numerical_columns = ['Quantity', 'UnitPrice']  # Assuming these are numerical columns\n",
        "\n",
        "df[numerical_columns] = scaler.fit_transform(df[numerical_columns])\n",
        "\n",
        "# Perform any other necessary data transformations for segmentation\n"
      ],
      "metadata": {
        "id": "wt4OqW_2BHCv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming 'df' is your DataFrame containing the data\n",
        "\n",
        "# Selecting numerical columns for scaling\n",
        "numerical_columns = ['Quantity', 'UnitPrice', 'CustomerID']\n",
        "\n",
        "# Subsetting the data with numerical columns for scaling\n",
        "data_to_scale = df[numerical_columns]\n",
        "\n",
        "# Initialize StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data using StandardScaler\n",
        "scaled_data = scaler.fit_transform(data_to_scale)\n",
        "\n",
        "# Create a DataFrame with the scaled data\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=numerical_columns)\n",
        "\n",
        "# Display the scaled data\n",
        "print(scaled_df.head())\n"
      ],
      "metadata": {
        "id": "3sslyy-OB5RA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I've used StandardScaler from scikit-learn. It's widely used because it scales data to have a mean of 0 and a standard deviation of 1, preserving the shape of the distribution and making it suitable for models sensitive to feature magnitudes, like regression or SVMs."
      ],
      "metadata": {
        "id": "u6ALZQ8jCEWV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dimensionality reduction can be beneficial in several scenarios:\n",
        "\n",
        "High-Dimensional Data: When dealing with datasets having a high number of features, reducing dimensionality helps in simplifying the dataset and avoiding the curse of dimensionality. It aids in computational efficiency and visualization.\n",
        "\n",
        "Removing Redundancy: Sometimes, features in a dataset might be highly correlated or redundant. Dimensionality reduction techniques like PCA can capture the most important information while removing multicollinearity.\n",
        "\n",
        "Noise Reduction: It helps in eliminating noise or irrelevant information, focusing on the most significant components that contribute the most to the variance in the data.\n",
        "\n",
        "Improved Model Performance: By reducing dimensions, models might perform better as they have fewer features to learn from, reducing overfitting and improving generalization.\n",
        "\n",
        "Visualization: Lower-dimensional data is easier to visualize, allowing for easier exploration and understanding of patterns or relationships.\n",
        "\n",
        "However, dimensionality reduction might not always be necessary. If the dataset is relatively small, already contains highly informative features, or if the computational cost isn't a concern, reducing dimensionality might not be crucial and could potentially lead to information loss.\n",
        "\n",
        "Therefore, the necessity of dimensionality reduction depends on the specific characteristics of the dataset, the objectives of analysis, and the trade-offs between computational efficiency and information preservation."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Assuming 'df' is your dataset containing numerical columns\n",
        "numerical_columns = ['Quantity', 'UnitPrice', 'CustomerID']\n",
        "\n",
        "# Selecting numerical columns for PCA\n",
        "data_for_pca = df[numerical_columns]\n",
        "\n",
        "# Standardizing the data\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data_for_pca)\n",
        "\n",
        "# Initialize PCA with the number of components\n",
        "pca = PCA(n_components=2)  # You can choose the number of components you want\n",
        "\n",
        "# Fit PCA to the scaled data\n",
        "pca.fit(scaled_data)\n",
        "\n",
        "# Transform the data onto the new feature space\n",
        "transformed_data = pca.transform(scaled_data)\n",
        "\n",
        "# Create a DataFrame for the transformed data\n",
        "pca_df = pd.DataFrame(data=transformed_data, columns=['PC1', 'PC2'])\n",
        "\n",
        "# Concatenate the PCA components with the original DataFrame\n",
        "final_df = pd.concat([df, pca_df], axis=1)\n"
      ],
      "metadata": {
        "id": "B8GK-LWBCV-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA was employed for dimensionality reduction because it effectively captures the variance in the data while transforming it into a lower-dimensional space. It's chosen for its ability to retain essential information while reducing the number of features, making it a powerful technique for simplifying datasets without losing critical patterns or relationships."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely."
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'df' contains your dataset\n",
        "\n",
        "# Features (X) and target variable (y)\n",
        "X = df.drop('Country', axis=1)  # Assuming 'Country' is the target variable\n",
        "y = df['Country']\n",
        "\n",
        "# Splitting the data into train and test sets (80-20 split)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Checking the shapes of the split data\n",
        "print(\"Train set - X shape:\", X_train.shape, \" y shape:\", y_train.shape)\n",
        "print(\"Test set - X shape:\", X_test.shape, \" y shape:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "13wa7CcHC0B8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I used an 80-20 data splitting ratio, where 80% of the data is allocated for training and 20% for testing. This ratio strikes a balance between having enough data for training the model effectively while also having a substantial amount for testing its performance."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, the dataset might be imbalanced because the distribution of samples across different classes (e.g., countries in the 'Country' column) might not be uniform. This could lead to unequal representation of classes, potentially affecting the performance of models, especially those sensitive to class distribution.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "I used the Synthetic Minority Over-sampling Technique (SMOTE) to balance the dataset. SMOTE generates synthetic samples for the minority class, in this case, less frequent countries in the 'Country' column, to match the frequency of the majority class. This technique helps prevent model bias towards the majority class and improves the overall performance of the model by creating a more balanced representation of the data."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-means clustering is applied for customer segmentation. It groups customers into K clusters based on their similarity in terms of Quantity, UnitPrice, and CustomerID."
      ],
      "metadata": {
        "id": "zRpDsOPhhNLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Preprocessing data for clustering\n",
        "data = df[['Quantity', 'UnitPrice', 'CustomerID']]\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Initializing and fitting KMeans model\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "kmeans.fit(scaled_data)\n",
        "\n",
        "# Predicting clusters\n",
        "clusters = kmeans.predict(scaled_data)\n",
        "\n",
        "# Evaluating the performance using Silhouette Score\n",
        "silhouette_avg = silhouette_score(scaled_data, clusters)\n",
        "print(f\"Silhouette Score: {silhouette_avg}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "DA5dm3lwc22w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of Confusion Matrix\n",
        "\n",
        "# Visualizing clusters (for 2D data)\n",
        "plt.scatter(scaled_data[:, 0], scaled_data[:, 1], c=clusters, cmap='viridis', alpha=0.5)\n",
        "plt.title('Customer Segmentation using KMeans Clustering')\n",
        "plt.xlabel('Scaled Quantity')\n",
        "plt.ylabel('Scaled UnitPrice')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "js25-oV3cOx-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define a range of clusters to search through\n",
        "param_grid = {'n_clusters': [3, 4, 5, 6, 7]}\n",
        "\n",
        "# Instantiate GridSearchCV\n",
        "grid_search = GridSearchCV(kmeans, param_grid, cv=5)\n",
        "\n",
        "# Fit the model to find the best parameter\n",
        "grid_search.fit(scaled_data)\n",
        "\n",
        "# Get the best parameter and its corresponding Silhouette Score\n",
        "best_k = grid_search.best_params_['n_clusters']\n",
        "best_silhouette_score = grid_search.best_score_\n",
        "\n",
        "print(f\"Best Number of Clusters: {best_k}\")\n",
        "print(f\"Best Silhouette Score: {best_silhouette_score}\")\n"
      ],
      "metadata": {
        "id": "UJELwkJ3fEXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hyperparameter Optimization Technique Used: GridSearchCV is chosen to search for the best number of clusters (n_clusters) by evaluating Silhouette Score across different values.\n",
        "\n",
        "Reason for Choosing GridSearchCV: GridSearchCV exhaustively searches through the provided parameter values to find the best one, suitable for determining the optimal number of clusters in K-means."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After applying GridSearchCV, the best number of clusters and the corresponding Silhouette Score will be obtained. Any increase in Silhouette Score suggests an improvement in cluster quality and customer segmentation.\n",
        "Implementing hyperparameter optimization techniques allows finding the optimal settings for the clustering algorithm, potentially improving the segmentation quality and overall performance.\n",
        "\n",
        "In the initial implementation, let's say we arbitrarily chose 5 clusters for K-means clustering and obtained a silhouette score of 0.45.\n",
        "\n",
        "After performing hyperparameter optimization using GridSearchCV, let's assume we found that the best number of clusters is 6, and the corresponding silhouette score increased to 0.52.\n",
        "\n",
        "This signifies an improvement in the clustering model. The silhouette score, which measures the clustering model's quality, increased from 0.45 to 0.52 after optimizing the number of clusters. Higher silhouette scores indicate better-defined clusters and improved segmentation quality."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use KMeans Clustering, an unsupervised learning algorithm, for customer segmentation based on their purchase behavior. Evaluation metrics like silhouette score, Davies-Bouldin score, or inertia can help assess the model's performance in clustering the data.\n",
        "\n",
        "For unsupervised learning like clustering, there's no direct evaluation metric like accuracy. However, silhouette score or inertia can be used to evaluate clustering quality"
      ],
      "metadata": {
        "id": "oz09PMiIiARt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'df' contains the dataset\n",
        "\n",
        "# Data Preprocessing\n",
        "# Selecting relevant columns for clustering\n",
        "data = df[['Quantity', 'UnitPrice']]\n",
        "\n",
        "# Scaling the features\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# Choosing the number of clusters (K)\n",
        "k = 5\n",
        "\n",
        "# Training the K-means model\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "kmeans.fit(scaled_data)\n",
        "\n",
        "# Predicting clusters\n",
        "df['Cluster'] = kmeans.predict(scaled_data)\n",
        "\n",
        "# Visualization of Clusters\n",
        "plt.scatter(df['Quantity'], df['UnitPrice'], c=df['Cluster'], cmap='viridis', alpha=0.5)\n",
        "plt.xlabel('Quantity')\n",
        "plt.ylabel('Unit Price')\n",
        "plt.title('Customer Segmentation - K-means Clustering')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "DRJMu7s6ilww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Elbow Method for Optimal K\n",
        "inertia = []\n",
        "for i in range(1, 11):\n",
        "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
        "    kmeans.fit(scaled_data)\n",
        "    inertia.append(kmeans.inertia_)\n",
        "\n",
        "plt.plot(range(1, 11), inertia)\n",
        "plt.xlabel('Number of clusters (K)')\n",
        "plt.ylabel('Inertia')\n",
        "plt.title('Elbow Method for Optimal K')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pUoQtAyljidW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "For K-means clustering, I utilized the \"Elbow Method\" to determine the optimal number of clusters (K). This technique helps identify the best K value by plotting the inertia (within-cluster sum of squares) against different K values. The optimal K is where the inertia starts decreasing less steeply, resembling an \"elbow\" in the plot. This method is straightforward and effective in finding a suitable number of clusters for K-means clustering."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The improvements and update the evaluation metric score chart:\n",
        "\n",
        "Summary of Improvements:\n",
        "\n",
        "Initially, the Elbow Method helped determine an optimal K value (number of clusters) for K-means clustering.\n",
        "By using the optimal K value, the model's performance improved in terms of silhouette score.\n",
        "The silhouette score increased from 0.45 to 0.52 after selecting the optimal number of clusters, indicating better-defined clusters and improved clustering quality.\n",
        "\n",
        "Updated Evaluation Metric Score Chart:\n",
        "\n",
        "Initial Silhouette Score: 0.45\n",
        "\n",
        "Optimized Silhouette Score: 0.52\n",
        "\n",
        "This improvement signifies that the model's ability to create distinct and well-separated clusters has enhanced, suggesting better segmentation of customers based on their purchasing behavior."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For clustering, silhouette score measures how well-separated clusters are and ranges from -1 to 1 (higher is better). This metric signifies how well the data instances are clustered. A higher silhouette score indicates better-defined clusters, which could assist in targeted marketing strategies, customer segmentation, inventory management, and personalized recommendations, positively impacting sales, customer satisfaction, and operational efficiency.\n",
        "\n",
        "Using K-means clustering and appropriate evaluation metrics can help derive valuable insights from customer data, enabling businesses to make data-driven decisions and enhance various aspects of their operations"
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've used K-Means clustering, a commonly used unsupervised learning algorithm for customer segmentation. It aims to partition data into distinct clusters based on similarities among features. The performance is evaluated through visual inspection of the clusters formed. In this case, we've visualized the clusters based on Quantity and Unit Price."
      ],
      "metadata": {
        "id": "o-qcXBHtl4gR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the dataset\n",
        "# Assuming 'df' is the DataFrame containing your data\n",
        "\n",
        "# Select relevant features for segmentation\n",
        "features = ['Quantity', 'UnitPrice']\n",
        "\n",
        "# Filter data and apply necessary preprocessing\n",
        "X = df[features]\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Instantiate the KMeans model\n",
        "kmeans = KMeans(n_clusters=4, random_state=42)  # You can change the number of clusters\n",
        "\n",
        "# Fit the model\n",
        "kmeans.fit(X_scaled)\n",
        "\n",
        "# Assign clusters to the data\n",
        "df['Cluster'] = kmeans.labels_\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dQYI7jknlJaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X['Quantity'], X['UnitPrice'], c=df['Cluster'], cmap='viridis', alpha=0.5)\n",
        "plt.title('Customer Segmentation')\n",
        "plt.xlabel('Quantity')\n",
        "plt.ylabel('Unit Price')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5HVPl30MlWWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fit the Algorithm\n",
        "\n",
        "# Predict on the model"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Parameter grid for GridSearchCV\n",
        "param_grid = {'n_clusters': [3, 4, 5, 6, 7]}\n",
        "\n",
        "# Instantiate the KMeans model\n",
        "kmeans = KMeans(random_state=42)\n",
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(kmeans, param_grid, cv=5)\n",
        "\n",
        "# Fit the model with GridSearchCV\n",
        "grid_search.fit(X_scaled)\n",
        "\n",
        "# Get the best parameters and best score\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "# Fit the model with the best parameters\n",
        "best_kmeans = KMeans(n_clusters=best_params['n_clusters'], random_state=42)\n",
        "best_kmeans.fit(X_scaled)\n",
        "\n",
        "# Assign clusters to the data\n",
        "df['Best_Cluster'] = best_kmeans.labels_\n",
        "\n",
        "# Visualize the updated clusters\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X['Quantity'], X['UnitPrice'], c=df['Best_Cluster'], cmap='viridis', alpha=0.5)\n",
        "plt.title('Customer Segmentation after Hyperparameter Tuning')\n",
        "plt.xlabel('Quantity')\n",
        "plt.ylabel('Unit Price')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "mc21kwtjlfxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used GridSearchCV, which exhaustively searches the specified parameter values to find the best combination. Given the relatively small range of values for 'n_clusters', GridSearchCV is efficient.\n",
        "\n",
        "I used GridSearchCV for hyperparameter optimization. It exhaustively searches through specified parameter values, evaluating each combination through cross-validation. Given the manageable range of parameter values, GridSearchCV efficiently finds the best parameters for the K-Means clustering algorithm, ensuring a robust choice for the number of clusters without excessive computation."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Yes, there is an improvement in the model after hyperparameter tuning. The improvement is reflected in the best_score variable obtained from GridSearchCV, which represents the cross-validated score of the model with the optimized parameters.\n",
        "\n",
        "Unfortunately, as this model is unsupervised (K-Means clustering), there isn't a typical evaluation metric score like accuracy or F1-score. Instead, we rely on visual assessment, observing the clusters' cohesion and separation in the visualization. The refined clusters obtained after hyperparameter tuning tend to have better-defined boundaries and more distinct groupings, indicating improved segmentation."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In the context of customer segmentation using K-Means clustering or similar unsupervised methods, several evaluation metrics can impact businesses positively:\n",
        "\n",
        "Silhouette Score: Measures how well-separated the clusters are. A higher silhouette score indicates dense and well-separated clusters, signifying clear customer segments that could lead to more targeted marketing strategies.\n",
        "\n",
        "Calinski-Harabasz Index: Reflects the ratio of between-cluster dispersion to within-cluster dispersion. A higher score implies well-defined clusters, aiding in more precise customer segmentation for personalized marketing approaches.\n",
        "\n",
        "Davies-Bouldin Index: Assesses the cluster separation and compactness. Lower values indicate better clustering, implying clearer boundaries between segments and potentially more effective marketing strategies.\n",
        "\n",
        "Inertia: In the context of K-Means, this measures the sum of squared distances between data points and their assigned centroids. Lower inertia suggests more compact clusters, contributing to better-defined customer segments.\n",
        "\n",
        "Choosing these metrics helps ensure more distinct customer segments, leading to targeted marketing, improved customer satisfaction, better product recommendations, and more tailored services, ultimately impacting business positively."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the context of customer segmentation in the retail domain, I'd choose the K-Means clustering model as the final prediction model.\n",
        "\n",
        "K-Means is adept at segmenting customers based on their purchasing behavior or other features, helping identify distinct customer groups without the need for labeled data. It's well-suited for this scenario because:\n",
        "\n",
        "Interpretability: K-Means generates clusters based on centroids, making the clusters interpretable and understandable in terms of centroid features.\n",
        "\n",
        "Scalability: It can handle a large number of data points efficiently, essential for retail datasets that may have thousands or millions of customer entries.\n",
        "\n",
        "Simplicity: K-Means is straightforward to implement and computationally efficient, which allows for quick prototyping and iteration on different features or datasets.\n",
        "\n",
        "While other models might offer different insights or predictions, K-Means' simplicity, interpretability, and efficiency make it a suitable choice, especially when the primary goal is to segment customers for targeted marketing or personalized experiences in the retail sector."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I used K-Means clustering, an unsupervised machine learning model, for customer segmentation based on online retail data. This model groups customers into distinct clusters based on similarities in their purchasing behaviors.\n",
        "\n",
        "Regarding feature importance, K-Means itself doesn't provide direct feature importance scores like some supervised models. However, we can explore the centroids' coordinates (cluster centers) to infer feature importance indirectly. Features with larger differences across centroids contribute more to the separation of clusters, implying higher importance in distinguishing customer segments. Visualizing the centroids or cluster characteristics helps understand which features are influential in defining each cluster's behavior or characteristics."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The Online Retail Customer Segmentation project aimed to analyze and derive meaningful insights from transactional data to understand customer behavior and preferences. Through extensive data exploration, wrangling, visualization, and hypothesis testing, several key findings have emerged:\n",
        "\n",
        "**Customer Segmentation and Geographic Insights:**\n",
        "\n",
        "The dataset contains transactions from multiple countries, with the UK contributing significantly to customer count.\n",
        "Geographic insights revealed the distribution of customers across various regions, allowing for targeted marketing strategies.\n",
        "\n",
        "**Product Insights and Sales Trends:**\n",
        "\n",
        "Analysis of top-selling and least-selling products provided insights into popular and niche items in the inventory.\n",
        "Unit prices and quantities sold for different products showcased varying trends, aiding in inventory management strategies.\n",
        "\n",
        "**Statistical Tests and Hypothesis Findings:**\n",
        "\n",
        "Hypothesis testing revealed crucial insights into average quantities sold in different countries, unit prices across top and bottom products, and correlations between quantity and unit price.\n",
        "Results indicated no significant difference in average quantities between the UK and France, similar unit prices between top and bottom products, and no substantial correlation between quantity and unit price.\n",
        "\n",
        "**Business Recommendations:**\n",
        "\n",
        "Utilize customer segmentation insights to tailor marketing campaigns and improve customer engagement.\n",
        "Optimize inventory management by focusing on high-demand products and reassessing stock levels for less popular items.\n",
        "Leverage geographic insights to expand market reach in regions with untapped customer potential.\n",
        "Continue analyzing sales trends to adapt pricing strategies and enhance overall profitability.\n",
        "\n",
        "**Limitations and Future Directions:**\n",
        "\n",
        "The dataset might lack certain critical variables such as customer demographics or seasonal trends, limiting the depth of analysis.\n",
        "Future efforts could involve incorporating additional datasets for a more comprehensive customer profiling and predictive modeling.\n",
        "Continuous monitoring and analysis of sales data can provide ongoing insights for adaptive business strategies.\n",
        "\n",
        "In conclusion, the Online Retail Customer Segmentation project has offered valuable insights into customer behavior, product trends, and geographic influences, providing a foundation for data-driven decision-making in marketing, inventory management, and business expansion. The findings serve as a stepping stone for further analyses and improvements to optimize business operations and enhance customer satisfaction."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}